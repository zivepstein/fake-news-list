{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A - David Lazer – looks at sites that snopes.com articles link to; manually classify into categories of green (18), yellow (24), orange (47), red (61), or satire (5). \n",
    "A = pd.read_csv(\"/Users/zive/GDrive/research/hcl/twitter-nudge/FN-agg/ASnopes domains annotated with urls.csv\")\n",
    "A = A[1:170] #remove empty rows\n",
    "A.columns = [d if d == \"Domain\" else \"A \" + d for d in A.columns]\n",
    "A[\"Domain\"] = A[\"Domain\"].str.lower()\n",
    "A['A_satire'] = [int(r == 'Satire') for r in A['A Likelihood  rating'].values]\n",
    "A['A_sketchy'] = [int(r == 'Orange') for r in A['A Likelihood  rating'].values]\n",
    "A['A_fake'] = [int(r == 'Red') for r in A['A Likelihood  rating'].values]\n",
    "\n",
    "#B -  Kate Starbird lists these sites that she has used in a paper at ICWSM and elsewhere: https://docs.google.com/spreadsheets/d/1lk3pFSc5wo3OfJc8ekONqO3MJCCigqe8SBSYwLYlHLo/edit#gid=0\n",
    "#(see here for more info: https://medium.com/hci-design-at-uw/information-wars-a-window-into-the-alternative-media-ecosystem-a1347f32fd8f )\n",
    "B = pd.read_csv(\"/Users/zive/GDrive/research/hcl/twitter-nudge/FN-agg/BAlternative Domains - Raw Data & Initial Coding - Sheet1.csv\")\n",
    "B.columns = [d if d == \"Domain\" else \"B \" + d for d in B.columns]\n",
    "B[\"Domain\"] = B[\"Domain\"].str.lower()\n",
    "B['B_satire'] = np.zeros(B.shape[0])  #TODO her methodology doesnt shed light on satire/sketchy/fake\n",
    "B['B_sketchy'] = np.zeros(B.shape[0])  #TODO  her methodology doesnt shed light on satire/sketchy/fake\n",
    "B['B_fake'] = np.zeros(B.shape[0])  #TODO  her methodology doesnt shed light on satire/sketchy/fake\n",
    "\n",
    "#https://www.buzzfeed.com/craigsilverman/top-fake-news-of-2016?utm_term=.kpl9an9AL#.aj4qmkqjo\n",
    "# this analysis focused exclusively on stories that were 100% false and that originated on fake news websites — it did not include misreported news or partisan misrepresentations of real events.\n",
    "Ca = pd.read_csv(\"/Users/zive/GDrive/research/hcl/twitter-nudge/FN-agg/C2017-12-fake-news-top-50/data/sites_2016.csv\")\n",
    "Ca = Ca.rename(columns= {\"domain\":\"Domain\"})\n",
    "Ca.columns = [d if d == \"Domain\" else \"Ca \" + d for d in Ca.columns]\n",
    "Ca[\"Domain\"] = Ca[\"Domain\"].str.lower()\n",
    "\n",
    "Cb = pd.read_csv(\"/Users/zive/GDrive/research/hcl/twitter-nudge/FN-agg/C2017-12-fake-news-top-50/data/sites_2017.csv\")\n",
    "Cb = Cb.rename(columns= {\"domain\":\"Domain\"})\n",
    "Cb.columns = [d if d == \"Domain\" else \"Cb \" + d for d in Cb.columns]\n",
    "Cb[\"Domain\"] = Cb[\"Domain\"].str.lower()\n",
    "\n",
    "C = pd.merge(Ca,Cb, on = \"Domain\", how = \"outer\")\n",
    "C['C_satire'] = np.zeros(C.shape[0]) \n",
    "C['C_sketchy'] = np.zeros(C.shape[0]) \n",
    "C['C_fake'] = np.ones(C.shape[0])\n",
    "\n",
    "#Craig Silverman and colleagues at Buzzfeed have curated 667 “hyperpartisan” sites related to the US election, and coded them as “left” and “right” leaning. \n",
    "#https://github.com/BuzzFeedNews/2017-08-partisan-sites-and-facebook-pages/tree/master/data\n",
    "#C3 hyperpartisan  = sketchy?\n",
    "C1 = pd.read_csv(\"/Users/zive/GDrive/research/hcl/twitter-nudge/FN-agg/C3all_partisan_sites.csv\")\n",
    "C1 = C1.rename(columns= {\"site\":\"Domain\"})\n",
    "C1.columns = [d if d == \"Domain\" else \"C1 \" + d for d in C1.columns]\n",
    "C1[\"Domain\"] = C1[\"Domain\"].str.lower()\n",
    "C1['C1_satire'] = np.zeros(C1.shape[0])\n",
    "C1['C1_sketchy'] = np.ones(C1.shape[0])\n",
    "C1['C1_fake'] =np.zeros(C1.shape[0])\n",
    "\n",
    "#D - Melissa took the criticism and revised with labels like unreliable, bias (used for Breitbart) and fake (bostonleader.com) and satirical etc (bostontribune.com)\n",
    "#her list:  https://docs.google.com/document/d/10eA5-mCZLSS4MQY5QGb5ewC3VAL6pLkT53V_81ZyitM/preview \n",
    "#article she wrote: https://www.washingtonpost.com/posteverything/wp/2016/11/18/my-fake-news-list-went-viral-but-made-up-stories-are-only-part-of-the-problem/?utm_term=.856e9fe7bef4\n",
    "#E is a  subset of D. E is their final polished list, and D is a larger, rougher list. \n",
    "#contains duplicates which are dropped in next cell\n",
    "D = pd.read_csv(\"/Users/zive/GDrive/research/hcl/twitter-nudge/FN-agg/DSearachable Spreadsheet Database - Sheet1.csv\")\n",
    "D = D.rename(columns= {\"url\":\"Domain\"})\n",
    "D.columns = [d if d == \"Domain\" else \"D \" + d for d in D.columns]\n",
    "sketchy_types=['bias', 'conspiracy', 'unreliable', 'clickbait', 'junksci', 'hate', 'rumor']\n",
    "D[\"Domain\"] = D[\"Domain\"].str.lower()\n",
    "D['D_satire'] = ((D[\"D type\"] == 'satire') | (D[\"D 2nd type\"] == 'satire') | (D[\"D 3rd type\"] == 'satire')).astype(int)\n",
    "D['D_fake'] = ((D[\"D type\"] == 'fake') | (D[\"D 2nd type\"] == 'fake')| (D[\"D 3rd type\"] == 'fake')).astype(int)\n",
    "#political, relibable and unidentified\n",
    "D['D_sketchy'] = ( np.isin(D[\"D type\"], sketchy_types) | np.isin(D[\"D 2nd type\"], sketchy_types) | np.isin(D[\"D 3rd type\"], sketchy_types)).astype(int)\n",
    "\n",
    "# http://www.opensources.co/\n",
    "# E = pd.read_csv(\"/Users/zive/GDrive/research/hcl/twitter-nudge/FN-agg/Esources.csv\")\n",
    "# E = E.rename(columns= {'Unnamed: 0':\"Domain\"})\n",
    "# E.columns = [d if d == \"Domain\" else \"E \" + d for d in E.columns]\n",
    "# E['E_satire'] = (E[\"E type\"] == 'satire') | (E[\"E 2nd type\"] == 'satire')| (E[\"E 3rd type\"] == 'satire')\n",
    "# E['E_fake'] = (E[\"E type\"] == 'fake') | (E[\"E 2nd type\"] == 'fake')| (E[\"E 3rd type\"] == 'fake')\n",
    "# #political, relibable and unidentified\n",
    "# E['E_sketchy'] = np.logical_not(E['E_satire'] | E['E_fake'])\n",
    "\n",
    "#- Politifact produced a list: http://www.politifact.com/punditfact/article/2017/apr/20/politifacts-guide-fake-news-websites-and-what-they/ That list was last updated in November.\n",
    "#Some fake stories not counted at all. no sketchy. Case sensitive \n",
    "F= pd.read_csv(\"/Users/zive/GDrive/research/hcl/twitter-nudge/FN-agg/Fpolifact.csv\")\n",
    "F = F.rename(columns= {'Site name':\"Domain\"})\n",
    "F.columns = [d if d == \"Domain\" else \"F \" + d for d in F.columns]\n",
    "F[\"Domain\"] = F[\"Domain\"].str.lower()\n",
    "F['F_satire'] = [int('parody' in r or 'Parody' in r) for r in F['F Type of site'].values]\n",
    "F['F_fake'] = [int('fake news' in r or 'Fake news' in r or 'Imposter site' in r) for r in F['F Type of site'].values]\n",
    "F['F_sketchy'] = np.zeros(F.shape[0])\n",
    "\n",
    "#remove melissa items\n",
    "#buzz though all 6 and see if they are all basically fake, if so ciount thgat \n",
    "#https://docs.google.com/spreadsheets/d/1S5eDzOUEByRcHSwSNmSqjQMpaKcKXmUzYT6YlRy3UOg/edit#gid=1882442466\n",
    "# This is what we use for Hoaxy. It's not terribly up to date though. HTH\n",
    "#https://docs.google.com/spreadsheets/d/1S5eDzOUEByRcHSwSNmSqjQMpaKcKXmUzYT6YlRy3UOg/edit?usp=sharing\n",
    "G = pd.read_csv(\"/Users/zive/GDrive/research/hcl/twitter-nudge/FN-agg/GClaim Sources - Public List.csv\")\n",
    "G = G.rename(columns= {'Source':\"Domain\"})\n",
    "G.columns = [d if d == \"Domain\" else \"G \" + d for d in G.columns]\n",
    "G[\"Domain\"] = G[\"Domain\"].str.lower()\n",
    "G[(G['G Listed by Melissa Zimdars'] == \"No\") | ([d  in ['duffelblog.com', 'en.mediamass.net', 'thedcgazette.com'] for d in G['Domain']])]\n",
    "G['G_satire'] = np.zeros(G.shape[0])  #TODO where does hoaxy method come from?\n",
    "G['G_sketchy'] = np.zeros(G.shape[0])  #TODO where does hoaxy method come from?\n",
    "G['G_fake'] = np.ones(G.shape[0])  #TODO where does hoaxy method come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = [A,C,C1,D,F]\n",
    "df_list_s = [\"A\",\"C\",\"C1\", \"D\",\"F\"]\n",
    "df = df_list[0]\n",
    "for df_ in df_list[1:]:\n",
    "    df = pd.merge(df, df_, how = \"outer\", on = \"Domain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_,df_s in zip(df_list, df_list_s):\n",
    "    df[df_s] = [int(d in df_[\"Domain\"].values) for d in df[\"Domain\"]]\n",
    "df[\"count\"] = df[df_list_s].sum(axis=1)\n",
    "df[\"fake_count\"]    = df[\"A_fake\"].fillna(0)   +df[\"C_fake\"].fillna(0)   +df[\"C1_fake\"].fillna(0)   +df[\"D_fake\"].fillna(0)   +df[\"F_fake\"].fillna(0)   \n",
    "df[\"satire_count\"]  = df[\"A_satire\"].fillna(0) +df[\"C_satire\"].fillna(0) +df[\"C1_satire\"].fillna(0) +df[\"D_satire\"].fillna(0) +df[\"F_satire\"].fillna(0) \n",
    "df[\"sketchy_count\"] = df[\"A_sketchy\"].fillna(0)+df[\"C_sketchy\"].fillna(0)+df[\"C1_sketchy\"].fillna(0)+df[\"D_sketchy\"].fillna(0)+df[\"F_sketchy\"].fillna(0)\n",
    "df = df.sort_values(by=\"count\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/Users/zive/GDrive/research/hcl/twitter-nudge/FN-agg/agg_out021218.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bias', 'conspiracy', 'unreliable', 'clickbait', 'junksci', 'hate', 'rumor']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fake          236\n",
       "unknown       168\n",
       "bias          134\n",
       "conspiracy    122\n",
       "satire        109\n",
       "political      65\n",
       "unreliable     56\n",
       "clickbait      35\n",
       "junksci        32\n",
       "hate           29\n",
       "rumor          11\n",
       "reliable        3\n",
       "state           1\n",
       "Name: D type, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D[\"D type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1770"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.Domain.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1770"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
